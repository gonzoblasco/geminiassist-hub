// This is an AI-powered code! Do not edit, except under strict compliance with the GenKit usage instructions!
'use server';
/**
 * @fileOverview This file defines a Genkit flow for moderating AI agent responses.
 *
 * The flow takes a text input and uses Gemini to determine if the content is safe and appropriate.
 * It exports the ModerateAgentResponses function, along with its corresponding input and output types.
 *
 * @remarks
 *  - ModerateAgentResponses - Function to moderate AI agent responses.
 *  - ModerateAgentResponsesInput - Input type for the ModerateAgentResponses function.
 *  - ModerateAgentResponsesOutput - Output type for the ModerateAgentResponses function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const ModerateAgentResponsesInputSchema = z.object({
  text: z
    .string() // Ensure 'text' is always a string
    .describe('The text to be moderated for safety and appropriateness.'),
});

export type ModerateAgentResponsesInput = z.infer<
  typeof ModerateAgentResponsesInputSchema
>;

const ModerateAgentResponsesOutputSchema = z.object({
  isSafe: z
    .boolean()
    .describe('Whether the text is considered safe and appropriate.'),
  reason: z
    .string()
    .optional()
    .describe('The reason why the text is not considered safe, if applicable.'),
});

export type ModerateAgentResponsesOutput = z.infer<
  typeof ModerateAgentResponsesOutputSchema
>;

export async function moderateAgentResponses(
  input: ModerateAgentResponsesInput
): Promise<ModerateAgentResponsesOutput> {
  return moderateAgentResponsesFlow(input);
}

const moderateAgentResponsesPrompt = ai.definePrompt({
  name: 'moderateAgentResponsesPrompt',
  input: {schema: ModerateAgentResponsesInputSchema},
  output: {schema: ModerateAgentResponsesOutputSchema},
  prompt: `You are an AI content moderation expert.

You are provided with text generated by an AI agent. Your task is to determine if the text is safe and appropriate for users.

Consider the following factors when making your determination:

*   Does the text contain any hate speech, harassment, or threats?
*   Does the text contain any sexually explicit content?
*   Does the text contain any dangerous or harmful content?
*   Does the text contain any misinformation or disinformation?
*   Does the text violate any ethical guidelines or community standards?

If the text is safe and appropriate, return isSafe as true. If the text is not safe or appropriate, return isSafe as false and provide a reason.

Text: {{{text}}}`,
  config: {
    safetySettings: [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_HARASSMENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
    ],
  },
});

const moderateAgentResponsesFlow = ai.defineFlow(
  {
    name: 'moderateAgentResponsesFlow',
    inputSchema: ModerateAgentResponsesInputSchema,
    outputSchema: ModerateAgentResponsesOutputSchema,
  },
  async input => {
    const {output} = await moderateAgentResponsesPrompt(input);
    return output!;
  }
);
